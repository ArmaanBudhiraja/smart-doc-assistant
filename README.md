# ğŸ“„ ThinkPDF 
### Chat with your documents. Instantly.

ThinkPDF is an AI-powered **Retrieval-Augmented Generation (RAG)** application that allows users to upload PDF documents and ask natural language questions about them. The system generates accurate, context-aware answers grounded strictly in the content of the uploaded document.

The application is built using a **local LLM (LLaMA 3 8B)**, ensuring privacy, offline capability, and full control over data â€” with no reliance on external AI APIs.

---

## âœ¨ Features

- ğŸ“¤ **PDF Upload & Text Extraction**  
  Upload text-based PDF files and automatically extract their content.

- ğŸ§  **Context-Aware Question Answering (RAG)**  
  Answers are generated using only the retrieved document context, reducing hallucinations.

- ğŸ” **Source Grounding**  
  Each answer is backed by relevant chunks from the original document.

- ğŸ’¬ **ChatGPT-style Conversational UI**  
  Multi-turn chat interface with preserved conversation history.

- ğŸ§© **Local LLM (LLaMA 3 8B)**  
  Runs entirely on local infrastructure â€” no OpenAI or cloud dependency.

- ğŸ¨ **Modern Two-Column UI**  
  Clean interface with document metadata on the left and chat on the right.

---

## ğŸ—ï¸ System Architecture

Frontend (Next.js)

â”‚

â”œâ”€â”€ PDF Upload

â”œâ”€â”€ Chat UI

â”‚

â””â”€â”€ API Calls

      â†“

Backend (FastAPI)

â”‚

â”œâ”€â”€ PDF Ingestion
â”œâ”€â”€ Text Chunking
â”œâ”€â”€ Embeddings (Sentence Transformers)
â”œâ”€â”€ Vector Store (FAISS)
â””â”€â”€ LLaMA 3 (Answer Generation)

---

## ğŸ§  Tech Stack

### Frontend
- Next.js (App Router)
- React + TypeScript
- Custom CSS
- Lucide Icons

### Backend
- FastAPI
- SentenceTransformers
- FAISS (Vector Database)
- LLaMA 3 8B (Local Inference)

### ML / NLP
- Retrieval-Augmented Generation (RAG)
- Semantic Search
- Embedding-based Similarity Matching

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/ArmaanBudhiraja/smart-doc-assistant.git
cd smart-doc-assistant
```

### 2ï¸âƒ£To download and run Llama3 8B 
```bash
brew install ollama
brew services start ollama 
ollama pull llama3:8b
```

### 3ï¸âƒ£ Backend Setup

```bash
cd backend
pip install -r requirements.txt
```
### Start the backend server:
```bash
uvicorn main:app --reload
```
### Backend will be available at: 
```bash
http://127.0.0.1:8000
```

### 4ï¸âƒ£ Frontend Setup

```bash
cd frontend
npm install
npm run dev
```

### Frontend will be available at:
```bash
http://localhost:3000
```

---

## ğŸ§ª How It Works (RAG Flow)
1. User uploads a PDF

2. Text is extracted and chunked

3. Chunks are converted into embeddings

4. Embeddings are stored in FAISS

5. User asks a question

6. Relevant chunks are retrieved via semantic search

7. LLaMA 3 generates an answer using retrieved context

8. Answer and sources are returned to the UI

---

### ğŸ” Privacy & Security

- No external API calls

- No document data stored permanently

- Fully local inference

- Suitable for sensitive documents

---

### ğŸ‘¨â€ğŸ’» Author

Armaan Budhiraja
B.Tech Computer Science & Engineering
Vellore Institute of Technology, Vellore

GitHub: https://github.com/ArmaanBudhiraja

LinkedIn: https://linkedin.com/in/armaanbudhiraja